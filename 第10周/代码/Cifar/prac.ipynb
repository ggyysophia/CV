{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a229649d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T04:18:21.219370Z",
     "start_time": "2021-08-24T04:18:16.758888Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import Cifar10_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08522285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T15:29:05.984079Z",
     "start_time": "2021-08-23T15:29:05.979028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4cb7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T04:18:11.955787Z",
     "start_time": "2021-08-24T04:18:11.951514Z"
    }
   },
   "outputs": [],
   "source": [
    "max_steps = 4000\n",
    "batch_size = 100\n",
    "num_examples_for_eval = 10000 \n",
    "data_dir = 'Cifar_data/cifar-10-batches-bin/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc576bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T04:21:52.685405Z",
     "start_time": "2021-08-24T04:21:52.679805Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建一个函数，variable_with_weight_loss()\n",
    "# 作用\n",
    "# 1. 使用参数w1控制L2 loss的大小\n",
    "# 2. 使用函数tf.nn.l2_loss()计算权重L2 loss\n",
    "# 3. 使用tf.multiply()计算权重L2 loss 与w1的乘积， 并赋值给weights_loss\n",
    "# 4. 使用函数tf.add_to_collection()将最终的结果放在名为losses的集合里面，方\n",
    "# 便后面计算神经网络总体损失loss\n",
    "\n",
    "\n",
    "def variable_with_weight_loss(shape, stddev, w1):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if w1 is not None:\n",
    "        weights_loss = tf.multiply(tf.nn.l2_loss(var), w1, name=\"weights_loss\")\n",
    "        tf.add_to_collection('losses', weights_loss)\n",
    "    return var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0e94b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T04:18:32.099572Z",
     "start_time": "2021-08-24T04:18:31.861834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/snszz/PycharmProjects/CV/第10周/代码/Cifar/Cifar10_data.py:49: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /Users/snszz/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /Users/snszz/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /Users/snszz/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Users/snszz/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Users/snszz/PycharmProjects/CV/第10周/代码/Cifar/Cifar10_data.py:27: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From /Users/snszz/PycharmProjects/CV/第10周/代码/Cifar/Cifar10_data.py:58: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/snszz/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Filling queue with 4000 CIFAR images before starting to train.    This will take a few minutes.\n",
      "WARNING:tensorflow:From /Users/snszz/PycharmProjects/CV/第10周/代码/Cifar/Cifar10_data.py:78: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "Filling queue with 4000 CIFAR images before starting to train.    This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "images_train, labels_train = Cifar10_data.inputs(data_dir=data_dir,batch_size=batch_size,\n",
    "                                                distorted=True)\n",
    "images_test, labels_test = Cifar10_data.inputs(data_dir=data_dir, batch_size=batch_size,\n",
    "                                               distorted=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9d4a46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T04:18:37.622297Z",
     "start_time": "2021-08-24T04:18:37.615114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/5t/c61g0ww90lq59p8dyj60dz400000gn/T/ipykernel_2242/473371434.py:1: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "y = tf.placeholder(tf.int32, [batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66a88733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T04:53:05.685404Z",
     "start_time": "2021-08-24T04:53:05.503843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2 (1, 2, 2)\n",
      "[ 5  5  3 64]\n",
      "5\n",
      "[100  24  24  64]\n",
      "[1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# 创建第一个卷积层 shape=(kh, hw, ci, co)\n",
    "k1 = tf.constant([[[2,3],[3,4]]])\n",
    "\n",
    "s2 = k1.get_shape()  # k1.get_shape().as_list()\n",
    "print('s2', s2)\n",
    "kernel1 = variable_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, w1=0.0)\n",
    "conv1 = tf.nn.conv2d(x, kernel1, [1,1,1,1], padding=\"SAME\")  #  https://blog.csdn.net/theadore2017/article/details/107232208/\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(tf.shape(kernel1)))\n",
    "    print(sess.run(tf.shape(kernel1)[0]))\n",
    "    print(sess.run(tf.shape(conv1)))\n",
    "    \n",
    "    print(sess.run(tf.shape(k1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018eb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d(\n",
    "    input,   # \n",
    "    filter=None,\n",
    "    strides=None,\n",
    "    padding=None,\n",
    "    use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC',\n",
    "    dilations=[1, 1, 1, 1],\n",
    "    name=None,\n",
    "    filters=None,\n",
    ")\n",
    "\n",
    "Flattens the filter to a 2-D matrix with shape\n",
    "   `[filter_height * filter_width * in_channels, output_channels]`.\n",
    "2. Extracts image patches from the input tensor to form a *virtual*\n",
    "   tensor of shape `[batch, out_height, out_width,\n",
    "   filter_height * filter_width * in_channels]`.\n",
    "3. For each patch, right-multiplies the filter matrix and the image patch\n",
    "   vector.\n",
    "\n",
    "In detail, with the default NHWC format,\n",
    "\n",
    "    output[b, i, j, k] =\n",
    "        sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q]\n",
    "                        * filter[di, dj, q, k]\n",
    "\n",
    "Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\n",
    "horizontal and vertices strides, `strides = [1, stride, stride, 1]`."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
